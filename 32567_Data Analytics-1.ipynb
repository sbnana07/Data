{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1><center>SOFTWARE LAB - III : Data Science Assignment - 7</center></h1>\n",
        "<h2 align=\"right\">32567 Harshal Tak</h2>"
      ],
      "metadata": {
        "id": "7IHVMzB01OQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7) Text Analytics\n",
        "1. Extract Sample document and apply following document preprocessing methods:\n",
        "Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
        "2. Create representation of documents by calculating Term Frequency and Inverse\n",
        "DocumentFrequency"
      ],
      "metadata": {
        "id": "9fpqhS-11XHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECSthaK41RuE",
        "outputId": "981cdeda-6620-411a-cdc6-4e226deac6da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is a sample text.\" + \\\n",
        " \" Hello, world!\" + \\\n",
        " \"This is Harshal Tak\" + \\\n",
        " \"Reporting from PES Modern College of Engineering. I want to go home :(\"\n",
        "print(sent_tokenize(text))\n",
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lvug6r7Q2DGw",
        "outputId": "8527b8d9-4997-4457-c42a-b5ffd84a5c1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This is a sample text.', 'Hello, world!This is Harshal TakReporting from PES Modern College of Engineering.', 'I want to go home :(']\n",
            "['This', 'is', 'a', 'sample', 'text', '.', 'Hello', ',', 'world', '!', 'This', 'is', 'Harshal', 'TakReporting', 'from', 'PES', 'Modern', 'College', 'of', 'Engineering', '.', 'I', 'want', 'to', 'go', 'home', ':', '(']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPfTR3NI2s8u",
        "outputId": "597bda1b-9764-4130-f3e4-664f01ee5b75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "tokenized = sent_tokenize(text)\n",
        "\n",
        "for i in tokenized:\n",
        "  wordsList = nltk.word_tokenize(i)\n",
        "\n",
        "  wordsList = [w for w in wordsList if not w in stop_words]\n",
        "\n",
        "  tagged = nltk.pos_tag(wordsList)\n",
        "\n",
        "  print(tagged)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GmKpmd94JnN",
        "outputId": "6d4ac142-e851-4f01-9878-0fd08ad45bb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('This', 'DT'), ('sample', 'NN'), ('text', 'NN'), ('.', '.')]\n",
            "[('Hello', 'NNP'), (',', ','), ('world', 'NN'), ('!', '.'), ('This', 'DT'), ('Harshal', 'NNP'), ('TakReporting', 'NNP'), ('PES', 'NNP'), ('Modern', 'NNP'), ('College', 'NNP'), ('Engineering', 'NNP'), ('.', '.')]\n",
            "[('I', 'PRP'), ('want', 'VBP'), ('go', 'JJ'), ('home', 'NN'), (':', ':'), ('(', '(')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "text = \"This is an example sentence demonstrating stop word removal.\"\n",
        "filtered_text = [word for word in word_tokenize(text) if word.lower() not in stop_words]\n",
        "print(filtered_text)"
      ],
      "metadata": {
        "id": "HShn1J0v5MDk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4751527a-1c3b-4230-9bd7-c761eb13e6a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['example', 'sentence', 'demonstrating', 'stop', 'word', 'removal', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# create an object of class PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "print(porter.stem(\"play\"))\n",
        "print(porter.stem(\"playing\"))\n",
        "print(porter.stem(\"plays\"))\n",
        "print(porter.stem(\"played\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BWjpZEjlCap",
        "outputId": "e96e7b79-f74b-4614-8ccf-806583ad7bb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "play\n",
            "play\n",
            "play\n",
            "play\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# create an object of class WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(lemmatizer.lemmatize(\"plays\", 'v'))\n",
        "print(lemmatizer.lemmatize(\"played\", 'v'))\n",
        "print(lemmatizer.lemmatize(\"play\", 'v'))\n",
        "print(lemmatizer.lemmatize(\"playing\", 'v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkRBF63vlPME",
        "outputId": "b85b2904-6271-4c0a-968c-ede21baf1b1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "play\n",
            "play\n",
            "play\n",
            "play\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(porter.stem(\"Communication\"))\n",
        "print(lemmatizer.lemmatize(\"Communication\", 'v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z79bgHEolSof",
        "outputId": "8a7829e8-e399-42d2-e0b4-037e455d8aa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "commun\n",
            "Communication\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string = \"\"\"\n",
        "Washington (CNN)John Bolton had to go -- because he wanted to cancel President Donald Trump's worldwide reality show.\\nFor a time the now ex-national security adviser, who first caught Trump's eye with his tough talk on Fox News, was useful to the President -- sharing his desire to shake up the globe.\\nBut like everyone else in Trump's dysfunctional foreign policy team, Bolton wore out his welcome, standing in the way of his boss' impetuous instincts and seeking a share of the spotlight.\\nOnly in the bizarre Trump orbit could the exit of a national security adviser seen as an ideologue and aggressive hawk also be perceived in some ways as the removal of a stabilizing force. But he did have a view of American interests and the use of US power that while hardline was predictable and logical and positioned within the historic boundaries of US diplomacy.\\nLike everything in Trump's foreign policy, there is a political explanation for the latest storm that rocked the White House.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "qwcHfL6Flf1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# Replace all line breaks with a space\n",
        "string = re.sub('\\n', ' ', string)\n",
        "# Replace all special characters with a space\n",
        "string = re.sub('[^A-Za-z0-9]+', ' ', string)"
      ],
      "metadata": {
        "id": "x--ThpJ3l-cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace all single characters with a space\n",
        "string = re.sub(r'\\b[a-zA-Z]\\b', ' ', string)\n",
        "# Replace all double spaces with one space\n",
        "string = re.sub(' +', ' ', string)\n",
        "# Remove leading and trailing spaces\n",
        "string = string.strip()"
      ],
      "metadata": {
        "id": "TnVgCv8YmC94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make all text lower case\n",
        "string = string.lower()"
      ],
      "metadata": {
        "id": "pFzyhzSlmG3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = list(string.split(\" \"))\n",
        "word_count = {}\n",
        "for word in words:\n",
        "    if word not in word_count:\n",
        "        word_count[word] = 1\n",
        "    elif word in word_count:\n",
        "        word_count[word] += 1\n",
        "print(word_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHRt34x4mJis",
        "outputId": "68d3bca0-5673-4761-88bc-a04fccf0ad93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'washington': 1, 'cnn': 1, 'john': 1, 'bolton': 2, 'had': 1, 'to': 4, 'go': 1, 'because': 1, 'he': 2, 'wanted': 1, 'cancel': 1, 'president': 2, 'donald': 1, 'trump': 5, 'worldwide': 1, 'reality': 1, 'show': 1, 'for': 2, 'time': 1, 'the': 12, 'now': 1, 'ex': 1, 'national': 2, 'security': 2, 'adviser': 2, 'who': 1, 'first': 1, 'caught': 1, 'eye': 1, 'with': 1, 'his': 4, 'tough': 1, 'talk': 1, 'on': 1, 'fox': 1, 'news': 1, 'was': 2, 'useful': 1, 'sharing': 1, 'desire': 1, 'shake': 1, 'up': 1, 'globe': 1, 'but': 2, 'like': 2, 'everyone': 1, 'else': 1, 'in': 5, 'dysfunctional': 1, 'foreign': 2, 'policy': 2, 'team': 1, 'wore': 1, 'out': 1, 'welcome': 1, 'standing': 1, 'way': 1, 'of': 7, 'boss': 1, 'impetuous': 1, 'instincts': 1, 'and': 5, 'seeking': 1, 'share': 1, 'spotlight': 1, 'only': 1, 'bizarre': 1, 'orbit': 1, 'could': 1, 'exit': 1, 'seen': 1, 'as': 2, 'an': 1, 'ideologue': 1, 'aggressive': 1, 'hawk': 1, 'also': 1, 'be': 1, 'perceived': 1, 'some': 1, 'ways': 1, 'removal': 1, 'stabilizing': 1, 'force': 1, 'did': 1, 'have': 1, 'view': 1, 'american': 1, 'interests': 1, 'use': 1, 'us': 2, 'power': 1, 'that': 2, 'while': 1, 'hardline': 1, 'predictable': 1, 'logical': 1, 'positioned': 1, 'within': 1, 'historic': 1, 'boundaries': 1, 'diplomacy': 1, 'everything': 1, 'there': 1, 'is': 1, 'political': 1, 'explanation': 1, 'latest': 1, 'storm': 1, 'rocked': 1, 'white': 1, 'house': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import required module\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# assign documents\n",
        "d0 = 'Geeks for geeks'\n",
        "d1 = 'Geeks'\n",
        "d2 = 'r2j'\n",
        "\n",
        "# merge documents into a single corpus\n",
        "string = [d0, d1, d2]\n",
        "\n",
        "# create object\n",
        "tfidf = TfidfVectorizer()\n",
        "\n",
        "# get tf-df values\n",
        "result = tfidf.fit_transform(string)\n",
        "\n",
        "# get idf values\n",
        "#print('\\nidf values:')\n",
        "#for ele1, ele2 in zip(tfidf.get_feature_names(), tfidf.idf_):\n",
        "#\tprint(ele1, ':', ele2)\n",
        "\n",
        "# get indexing\n",
        "print('\\nWord indexes:')\n",
        "print(tfidf.vocabulary_)\n",
        "\n",
        "# display tf-idf values\n",
        "print('\\ntf-idf value:')\n",
        "print(result)\n",
        "\n",
        "# in matrix form\n",
        "print('\\ntf-idf values in matrix form:')\n",
        "print(result.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQzLaa7ymORU",
        "outputId": "76761995-b66e-4c9a-d885-c7d5ae821ae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word indexes:\n",
            "{'geeks': 1, 'for': 0, 'r2j': 2}\n",
            "\n",
            "tf-idf value:\n",
            "  (0, 0)\t0.5493512310263033\n",
            "  (0, 1)\t0.8355915419449176\n",
            "  (1, 1)\t1.0\n",
            "  (2, 2)\t1.0\n",
            "\n",
            "tf-idf values in matrix form:\n",
            "[[0.54935123 0.83559154 0.        ]\n",
            " [0.         1.         0.        ]\n",
            " [0.         0.         1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get indexing\n",
        "print('\\nWord indexes:')\n",
        "print(tfidf.vocabulary_)\n",
        "\n",
        "# display tf-idf values\n",
        "print('\\ntf-idf value:')\n",
        "print(result)\n",
        "\n",
        "# in matrix form\n",
        "print('\\ntf-idf values in matrix form:')\n",
        "print(result.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ouyNG5InIIN",
        "outputId": "cb69cf99-9f86-4437-a1b6-c8144e91761d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word indexes:\n",
            "{'geeks': 1, 'for': 0, 'r2j': 2}\n",
            "\n",
            "tf-idf value:\n",
            "  (0, 0)\t0.5493512310263033\n",
            "  (0, 1)\t0.8355915419449176\n",
            "  (1, 1)\t1.0\n",
            "  (2, 2)\t1.0\n",
            "\n",
            "tf-idf values in matrix form:\n",
            "[[0.54935123 0.83559154 0.        ]\n",
            " [0.         1.         0.        ]\n",
            " [0.         0.         1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "puUC0CbbnegF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}